# -*- coding: utf-8 -*-
"""Spam_ham_classifiaction_using_NPL_code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1I7xTF0KwbbAWOBedRdeSGMCtLE4zUCVI

# SMS SPAM Filter with Natural Language Processing !

# Section I Data Analysis

# Step 1:

### import all modules
"""

import nltk
from nltk.corpus import stopwords

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer

import pandas as pd
import string
import seaborn as sns

"""### Read the Dataset"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv("/content/drive/MyDrive/Spam_Ham/spam.csv", encoding = "ISO-8859-1")

df.head(20)

"""This gives us information about columns"""

df.info()
df.describe()

"""This gives us a general Idea about the Dataset"""

df.groupby('v1').describe()

"""This shows us how many Spam vd Normal Messages we have in the Dataset we have about 747 apam messages
and 4825 messages which are not spam and 4500 is uniqe messages from there

# Step 2:

### Lets us extract some Features as well
"""

df["Length"] = df["v2"].apply(len)

df.head(2)

"""### This will help our model to have more features so it can predict based on length of messages"""

sns.distplot(df["Length"], bins=30)

"""#### This tell us distribtion of length of messages

# Questions

### what is Maximum Length of Message some one sent and what is that message
"""

df["Length"].max()

df[df["Length"]==910]["v2"].iloc[0]

"""### Conclusion: The Maximum Length of message is 143 and it seems to be a love letter Lol !

### what is Mininum Length of Message some one sent and what is that message
"""

df[df["Length"] == df["Length"].min()]["v2"].iloc[0]

"""### Conclusion: The Mininum Length of message is 2 and it seems to be just OK"""

df.head(1)

sns.barplot(x='v1', y='Length', data=df)

df.hist(column='Length',by='v1', bins=50)

"""# Section II

### Remove Punctuation from message

### Small Library to Pre Process Text Data
"""

class PreProcessText(object):
    def __init__(self):
        pass

    def __remove_punctuation(self, text):
        """
        Takes a String
        return : Return a String
        """
        message = []
        for x in text:
            if x in string.punctuation:
                pass
            else:
                message.append(x)
        message = ''.join(message)

        return message

    def __remove_stopwords(self, text):
        """
        Takes a String
        return List
        """
        words= []
        for x in text.split():

            if x.lower() in stopwords.words('english'):
                pass
            else:
                words.append(x)
        return words


    def token_words(self,text=''):
        """
        Takes String
        Return Token also called  list of words that is used to
        Train the Model
        """
        message = self.__remove_punctuation(text)
        words = self.__remove_stopwords(message)
        return words

nltk.download('stopwords')
mess = "FreeMsg Hey there darling it's been 3 week's now and no word back! I'd like some fun you up for it still? Tb ok! XxX std chgs to send, å£1.50 to rcv"
obj = PreProcessText()
words = obj.token_words(mess)
print(words)



"""### we have converted our string into Token for the Model"""

df["v2"].head(4).apply(obj.token_words)

"""## Vectorization"""

bow_transformer = CountVectorizer(analyzer=obj.token_words).fit(df["v2"])

messages_bow = bow_transformer.transform(df["v2"])

print("Shape of sparese matrix ")

"""### Formula to check Sparsity"""

sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))
print('sparsity: {}'.format(sparsity))

"""#### Converted   word count into TFIDF(Term Frequency-Inverse Document Frequency) think this a weight"""

tfidf_transformer = TfidfTransformer().fit(messages_bow)

messages_tfidf = tfidf_transformer.transform(messages_bow)

"""#### Train the Model"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB().fit(messages_tfidf,df["v1"])

"""#### Prediction"""

all_predictions = model.predict(messages_tfidf)
pred = pd.DataFrame(data=all_predictions)

pred.head(6)

"""### True Value"""

df["v1"].head(6)

print('Actual values:')
print(df["v1"].head(20))
print('\n******************************************************\n')
print("Predicted values")
print(pred.head(20))

